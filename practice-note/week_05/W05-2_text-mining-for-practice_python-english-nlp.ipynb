{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT MINING for PRACTICE\n",
    "- 본 자료는 텍스트 마이닝을 활용한 연구 및 강의를 위한 목적으로 제작되었습니다.\n",
    "- 본 자료를 강의 목적으로 활용하고자 하시는 경우 꼭 아래 메일주소로 연락주세요.\n",
    "- 본 자료에 대한 허가되지 않은 배포를 금지합니다.\n",
    "- 강의, 저작권, 출판, 특허, 공동저자에 관련해서는 문의 바랍니다.\n",
    "- **Contact : ADMIN(admin@teanaps.com)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEEK 05-2. 영어 텍스트 데이터 전처리: NLTK\n",
    "- Python의 NLTK 패키지를 활용해 텍스트 데이터를 전처리하는 방법에 대해 다룹니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 영어 텍스트 데이터를 전처리하는 방법 알아보기: NLTK\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. TEANAPS 패키지 설치하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분석을 위한 TEANAPS 패키지를 설치합니다.\n",
    "# TEANAPS는 Google Colaboratory/Linux 환경에 최적화되어 있습니다.\n",
    "# Windows 환경에서 일부 기능에 제한이 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEANAPS (https://github.com/fingeredman/teanaps)\n",
    "#!git clone https://github.com/fingeredman/teanaps.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEANAPS 설치를 진행합니다.\n",
    "# 설치 전 반드시 상단 메뉴에서 [런타임 > 런타임 초기화]를 클릭한 후 진행해주세요.\n",
    "#!python \"teanaps/teanaps_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. 단어의 원형을 복원하는 첫 번째 방법 알아보기: Stemming\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cooking : cook\n",
      "2. cookery : cookeri\n",
      "3. believes : believ\n",
      "4. using : use\n"
     ]
    }
   ],
   "source": [
    "# NLTK 패키지에서는 세 가지 Stemming 도구를 제공합니다.\n",
    "# 각 Stemmeer는 Stemming을 하는 방법과 기준이 서로 다릅니다.\n",
    "# 각각의 Stemmer 마다 특성이 다르니 사용해보고 알맞은 Stemmer를 골라 사용해야합니다\n",
    "# 일반적으로 Porter Stemmer가 가장 많이 활용됩니다\n",
    "\n",
    "# 1) Porter Stemmer\n",
    "\n",
    "# Porter Stemmer를 불러옵니다.\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"1. cooking :\", ps.stem(\"cooking\"))\n",
    "print(\"2. cookery :\", ps.stem(\"cookery\"))\n",
    "print(\"3. believes :\", ps.stem(\"believes\"))\n",
    "print(\"4. using :\", ps.stem(\"using\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cooking : cook\n",
      "2. cookery : cookery\n",
      "3. believes : believ\n",
      "4. using : us\n"
     ]
    }
   ],
   "source": [
    "# 2) Lancaster Stemmer\n",
    "\n",
    "# Lancaster Stemmer를 불러옵니다.\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print(\"1. cooking :\", ls.stem(\"cooking\"))\n",
    "print(\"2. cookery :\", ls.stem(\"cookery\"))\n",
    "print(\"3. believes :\", ps.stem(\"believes\"))\n",
    "print(\"4. using :\", ls.stem(\"using\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cooking : cook\n",
      "2. cookery : cookery\n",
      "3. believes : lesidie\n",
      "4. using : us\n"
     ]
    }
   ],
   "source": [
    "# 2) Regexp Stemmer\n",
    "\n",
    "# Regexp Stemmer를 불러옵니다.\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing')\n",
    "\n",
    "print(\"1. cooking :\", rs.stem(\"cooking\"))\n",
    "print(\"2. cookery :\", rs.stem(\"cookery\"))\n",
    "print(\"3. believes :\", rs.stem(\"inglesidie\"))\n",
    "print(\"4. using :\", rs.stem(\"using\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.3. 단어의 원형을 복원하는 두 번째 방법 알아보기: Lemmatizing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing은 사전을 기반으로 단어의 원형을 찾아 복원합니다.\n",
    "# NLTK의 Lemmatizing 기능을 활용하기 위해서는 별도로 사전을 다운로드 받아야 합니다.\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1. cooking : cooking\n",
      "1-2. cooking(pos=\"v\") : cook\n",
      "1-3. cooking(pos=\"n\") : cooking\n",
      "2. cookbooks : cookbook\n",
      "3. believes : believe\n",
      "4. buses : bus\n",
      "5. using : use\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer를 불러옵니다.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizer는 lemmatize(WORD, pos=POS) 함수로 호출합니다.\n",
    "# pos 변수에는 v, n, a 세 가지 품사를 입력할 수 있습니다. (필수는 아님)\n",
    "print(\"1-1. cooking :\", lemmatizer.lemmatize(\"cooking\"))\n",
    "print(\"1-2. cooking(pos=\\\"v\\\") :\", lemmatizer.lemmatize(\"cooking\", pos=\"v\"))\n",
    "print(\"1-3. cooking(pos=\\\"n\\\") :\", lemmatizer.lemmatize(\"cooking\", pos=\"n\"))\n",
    "print(\"2. cookbooks :\", lemmatizer.lemmatize(\"cookbooks\"))\n",
    "print(\"3. believes :\", lemmatizer.lemmatize(\"believes\", pos=\"v\"))\n",
    "print(\"4. buses :\", lemmatizer.lemmatize(\"buses\"))\n",
    "print(\"5. using :\", lemmatizer.lemmatize(\"using\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. 단어의 원형을 복원하는 세 번째 방법 알아보기: Replacer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I should have done that thing I did not do.\n"
     ]
    }
   ],
   "source": [
    "# Replacer를 불러옵니다.\n",
    "from teanaps.nlp import Processing\n",
    "processing = Processing()\n",
    "\n",
    "# replace(SENTENCE) 함수는 문장 내 줄임말을 원래의 형태로 복원합니다.\n",
    "sentence = \"I should've done that thing I didn't do.\"\n",
    "new_sentence = processing.replacer(sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.5. 문단을 문장 단위로, 문장을 단어 단위로 분리하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World.', 'How are you?', \"It's good to see you.\", 'Thanks for buying this book.']\n"
     ]
    }
   ],
   "source": [
    "# 1) Sentence Tokenizer\n",
    "# Sentence Tokenizer를 불러옵니다.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "paragraph = \"Hello World. How are you? It's good to see you. Thanks for buying this book.\"\n",
    "# sent_tokenize(PARAGRAPH) 함수는 다수의 문장이 포함된 문단을 문장 단위로 잘라 리스트로 만듭니다.\n",
    "sentences_list = sent_tokenize(paragraph)\n",
    "print(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'good', 'to', 'see', 'you']\n"
     ]
    }
   ],
   "source": [
    "# 2) Word Tokenizer\n",
    "# Word Tokenizer를 불러옵니다.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"It's good to see you\"\n",
    "# word_tokenize(SENTENCE) 함수는 문장을 단어 단위로 잘라 리스트로 만듭니다.\n",
    "word_list = word_tokenize(sentence)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. 영문 형태소분석하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK의 형태소분석 기능을 활용하기 위해서는 별도의 다운로드가 필요합니다.\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Text', 'NN'), ('mining', 'NN'), ('is', 'VBZ'), ('difficult', 'JJ'), ('but', 'CC'), ('very', 'RB'), ('valuable', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "word_list = word_tokenize(\"Text mining is difficult but very valuable.\")\n",
    "# NLTK 형태소분석기의 pos_tag(WORD_LIST) 함수의 WORD_LIST에는 문자열(문장)이 아닌 단어단위로 구분된 리스트를 넣어줍니다.\n",
    "word_tagging_list = nltk.tag.pos_tag(word_list)\n",
    "print(word_tagging_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8. [1.1 ~ 1.7] 과정 활용하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World. It's good to see you. Thanks for buying this book.\n"
     ]
    }
   ],
   "source": [
    "# 1.1 ~ 1.5 과정을 순서대로 수행합니다.\n",
    "\n",
    "# 1. 원본문단\n",
    "paragraph = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world. it's good to see you. thanks for buying this book.\n"
     ]
    }
   ],
   "source": [
    "# 2. 문자열을 모두 소문자로 변경\n",
    "paragraph = paragraph.lower()\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world. it is good to see you. thanks for buying this book.\n"
     ]
    }
   ],
   "source": [
    "# 3. 문장의 줄임말을 원래의 형태로 복원\n",
    "paragraph = processing.replacer(paragraph)\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello world.', 'it is good to see you.', 'thanks for buying this book.']\n"
     ]
    }
   ],
   "source": [
    "# 4. 여러개 문장이 포함된 문단을 문장 단위로 구분\n",
    "sentence_list = sent_tokenize(paragraph)\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.']\n",
      "['it', 'is', 'good', 'to', 'see', 'you', '.']\n",
      "['thanks', 'for', 'buying', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# 5. 문장을 단어 단위로 구분\n",
    "token_sentence_list = []\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    word_list = word_tokenize(sentence)\n",
    "    token_sentence_list.append(word_list)\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'JJ'), ('world', 'NN'), ('.', '.')]\n",
      "[('it', 'PRP'), ('is', 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.')]\n",
      "[('thanks', 'NNS'), ('for', 'IN'), ('buying', 'VBG'), ('this', 'DT'), ('book', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 6. 형태소태깅\n",
    "tag_sentence_list = []\n",
    "\n",
    "for token_sentence in token_sentence_list:        \n",
    "    tag_list = nltk.tag.pos_tag(token_sentence)\n",
    "    tag_sentence_list.append(tag_list)\n",
    "    print(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.']\n",
      "['it', 'is', 'good', 'to', 'see', 'you', '.']\n",
      "['thank', 'for', 'buy', 'thi', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# 7-1. 원형복원: Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for tag_sentence in tag_sentence_list:\n",
    "    print([ps.stem(word) for word, pos in tag_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.']\n",
      "['it', 'be', 'good', 'to', 'see', 'you', '.']\n",
      "['thanks', 'for', 'buy', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# 7-2. 원형복원: Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for tag_sentence in tag_sentence_list:\n",
    "    lemma_sentence = []\n",
    "    for word, pos in tag_sentence:\n",
    "        # 품사가 형용사(a), 동사(v), 명사(n)인 경우에만 품사를 옵션으로 넣어줍니다.\n",
    "        if pos.lower()[0] in [\"a\", \"v\", \"n\"]:\n",
    "            lemma_sentence.append(lemmatizer.lemmatize(word, pos=pos.lower()[0]))\n",
    "        else:\n",
    "            lemma_sentence.append(lemmatizer.lemmatize(word))\n",
    "    print(lemma_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 영어 텍스트 데이터를 전처리하는 방법 알아보기: TEANAPS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. TEANAPS 패키지 내 형태소분석기 호출하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'VA', (0, 5)), ('world', 'NNG', (6, 11)), ('.', 'SW', (11, 12)), ('it', 'NP', (13, 15)), ('', 'VV', (0, 0)), ('good', 'VA', (19, 23)), ('to', 'TO', (24, 26)), ('see', 'VV', (27, 30)), ('you', 'NP', (31, 34)), ('.', 'SW', (34, 35)), ('thanks', 'NNG', (36, 42)), ('for', 'IN', (43, 46)), ('buy', 'VV', (47, 50)), ('this', 'DT', (54, 58)), ('book', 'NNG', (59, 63)), ('.', 'SW', (63, 64))]\n"
     ]
    }
   ],
   "source": [
    "# TEANAPS 형태소 분석기를 불러옵니다.\n",
    "from teanaps.nlp import MorphologicalAnalyzer\n",
    "\n",
    "ma = MorphologicalAnalyzer()\n",
    "\n",
    "# TEANAPS에서는 영어 텍스트 분석을 위해 NLTK의 형태소분석기를 지원합니다.\n",
    "# TEANAPS의 형태소분석 과정은 영어 텍스트에 필요한 전처리를 모두 포함합니다.\n",
    "\n",
    "text = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "pos_list = ma.parse(text)\n",
    "print(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
